\chapter{Introduction}
Within the recent years, we have seen an incredible growth in the gaming industry. Newzoo, a leading provider of games market intelligent, reported that “the gaming industry is growing faster than expected, up 10.7\% to \$116 billion 2017” \cite{takahashi_game_2017}. Most of the revenue comes from the mobile industry that accounts for almost half of the global game market and the perspectives show a continuous growth at an annual rate of 8.2\% to 2020. Everyday, more and more people use their smartphone to play mobile games and a recent report on Verto Analytics \cite{hwong_leveling_2016} showed that gamers play an average of 24 minutes each day. Being able to continuously engage users is a key capability that game companies need to possess. Another current trend is the shifting from the traditional \acf{P2P} business model to the \acf{F2P} or “freemium” business model. The \acs{F2P} business model allows users to acquire and play a game free of charge and at the same time encourages them to pay for in-game additional content. More than that, it allows game companies to increase the popularity of a game and to collect revenues in a second moment. This fundamental shift is even more preeminent in the mobile game industry as demonstrated by the success and highly profitable results of many \acs{F2P} mobile titles (see e.g. \cite{alha_critical_2016,alha_free--play_2014,fields_mobile_2014,civelek_pricing_2018,hamari_service_2017,hamari_why_2017}). 

To exploit the \acs{F2P} business model, game companies need to continuously release new content (e.g. levels, characters, items to collect) or add new features to engage users and retain them as long as possible. It is extremely important that the content generated by game designers matches the expectations of the players \cite{hamari_service_2017}. In this direction, one of the most important features is the user´s perceived difficulty of a game \cite{alexander_investigation_2013}. If a game is too hard, the player gets frustrated and contrariwise if a game is too easy the player gets bored. Being able to balance the difficulty of a game \cite{hunicke_case_2005}, and so its perceived quality, is the primary challenge that game designers need to deal with every day. Sometimes, game designer’s intuition is not enough to balance a game, especially for those features whose impact on the difficulty is not obvious. A common approach to balance a game is to iteratively test and adjust game content, like a level or a character, until a predefined design goal is achieved. 
% This approach can be fully automated, leading to \acf{PCG} \cite{hendrikx_procedural_2013, togelius_search-based_2011, risi_petalz:_2016, hastings_automatic_2009} or it can be used jointly with game designer to test new content and manually fine-tune it \cite{schell_art_2014}. 
% We assume that the second approach leads to better results, especially in casual games, because we believe that the creativity and passion that designers can put in a game is not equally expressed by machines. 

The focus of this research is on understanding and testing game content, leaving the creation process to game designers. Human testing has been widely used in the past but at the same time, it is expensive, time consuming and relies on subjective feedback \cite{zook_automatic_2014}. To solve this problem several approaches have been proposed, leading to what is called \acl{AP} \cite{silva_ai_2017}.

\section{Background} \label{Background_intro}

The work of this thesis can be classified under the broad category of \acf{AI}. \acs{AI} is understood as the study and development of "intelligent agents". Agents that mimics \textit{cognitive} functions of humans, like reasoning, learning or memory. Since its foundation at a workshop at Dartmouth College in 1956, \acs{AI} experienced phases of interest and funding as well as phases of critics and disappointments as demonstrated by the two major "AI winters" in 1974–80 and 1987–93. Since then, as machines have become more powerful, the definition of which task requires "knowledge" and as a consequence, what makes an agent "intelligent", has changed and it still continue to change. Nowadays, \acs{AI} is widely applied in different fields and disciplines and examples of actual challenges are: autonomous driving, executing medical diagnosis or competing with humans in games. Since the birth of \acs{AI}, games and especially board games, have been a popular domain of research \cite{yannakakis_artificial_2017} due to the formal and constrained environments, yet complex decision making problems. The combination of game and \acs{AI} is demonstrated by the increasing popularity of meetings, like the \textit{Artificial Intelligence and Interactive Digital Entertainment Conference (AIIDE)}, the \textit{Conference on Computational Intelligence and Games (CIG)} and the \textit{Game Developers Conference (GDC)}, or journals, like the  \textit{Transactions on Games (ToG)}, that continuously presents the major results in the field. Within the \acs{AI} area, this thesis involves three major categories: automatic playtesting, player modeling and gameplay simulation.


In the last years, automatic playtesting has become more and more popular thanks to the increasing performances of computational resources and the notable results in the machine learning field. \textcite{zook_automatic_2014} used active learning techniques to reduce the amount of playtesting needed to adjust game parameters. They reduced the cost of playtesting by intelligently picking the parameters to test, but they still used human testing to evaluate various parameter configurations. \textcite{isaksen_exploring_2015} used automatic playtesting and player modeling to tune game parameters to reach a predefined difficulty. They implemented a simple AI agent that performed better than humans in an action game that requires only motor skills but no strategy or planning. Then, they simulated humans by adding dexterity, reaction time and accuracy errors. Finally, they used survival analysis to explore the game space by adjusting low-level parameters of the game. \textcite{holmgard_evolving_2014} used linear perceptron and \acf{RL} to simulate a priori defined styles of playing. While later, \cite{holmgard_evolving_2016} they compared two different methods of representing human decision making styles. They illustrated that a top-down approach from expert knowledge performs equally good as a bottom-up approach from human play-traces. However their analysis is limited to a simple dungeon exploration game and there is no evidence that the same results apply for different or more complex games. \textcite{isaksen_simulating_2017} used variants of Q-learning-based agents to simulate several types of players with different skills. Subsequently they used the trained agents to estimate the strategy and dexterity required on each level. \textcite{drachen_player_2009} used self-organizing maps to construct models of players and inform game designers if players interact with the game as intended. \textcite{hoorn_robust_2009} used multiobjective evolutionary algorithms to train an agent that not only performs good in the game but  at the same time imitates human players. They concluded that imitating humans is a hard task and as a fact, their method performed poorly on this task. \textcite{eisen_simulating_2017} used a \acf{CNN} to predict human moves in the \textit{Candy Crush Saga} game. Consequently he used the \acs{CNN}-based agent to simulate gameplay and finally, using the agent performances he estimated difficulty of new levels. However, each player has his own way of reasoning and a game content can be perceived difficult by one player and easy by another one. As a consequence, modeling different types of players' behaviour can be beneficial to understand how different people interact with the game and which difficulties they face.
 
\section{Problem}
In the literature, many variants of playtesting have been discussed and investigated. However, some of the proposed approaches \cite{eisen_simulating_2017,hoorn_robust_2009,poromaa_crushing_2017,ortega_imitating_2013,hlynur_predicting_2017,purmonen_predicting_2017,chen_game_2017} consider difficulty as a single measure while, indeed, difficulty can be perceived differently by various type of players. Instead others \cite{silva_ai_2017,isaksen_exploring_2015,holmgard_evolving_2014,isaksen_simulating_2017,holmgard_generative_2014} model different AI agents to simulate different players but they do not consider real player data while learning the policies and rely on designers' knowledge or \acs{RL} to differentiate the agents. We believe that using \acl{PM} to simulate strategies of various cohorts of players while directly learning the polices from real player data can led to both a closer to human and heterogeneous simulation. Therefore, we can have a better estimate of the quality of a game, helping game designers to faster and more accurately develop the desired content.

To validate our idea, in this research we use the \textit{Candy Crush Saga} game as an example. However our approach is general and can be adapted to work with other games too.
At the moment of writing, there exists an implementation of a \ac{CNN}-based agent that simulates human gameplay to predict the player \acf{sr} for the selected game \cite{eisen_simulating_2017}. Since this method reached the best performance to date, we will use it as a baseline. However, the existing method uses only an average policy learnt from a random subsample of players, without considering any difference in the skills or strategies of different cohorts of players, nor any other information concerning them. We believe that being able to incorporate these characteristics of the players into the model allows to improve the prediction accuracy of the network as well as providing useful insight about the behaviour of the players and the characteristics of the levels. The measure used to evaluate the difficulty of a level in this thesis is the \acs{sr}. As a consequence, the research question examined is:
\begin{center}
        \textit{ Can \acl{PM} improve the players’ \acs{sr} estimation of a \acs{CNN}-based agent simulating human gameplay?} % in the Candy Crush Saga game
\end{center}
 

\section{Goals and Purpose}
The thesis illustrates how player modeling techniques can be used to model strategies and skills of different groups of players by simulating them using \ac{CNN}-based agents. The primary goal of the thesis is improving automatic playtesting using \acl{PM} techniques. We illustrate how the developed agents can be useful for predicting the human \acs{sr} on new levels for the game selected as test bench in this research. Subsequently, we aim to provide game designers with level difficulty measures perceived by the different groups of players to iteratively balance game content and create the best possible player experience. Furthermore, this work illustrates how the new approach outperforms the previous state-of-the-art \cite{eisen_simulating_2017} in the human SR estimation task. Finally, we discuss how simulating different human gameplay can provide valuable insights about the behaviours of the players and the content of the game.



\subsection{Benefits, Ethics and Sustainability}
Being able to model different types of players can be beneficial for all the companies in the game industry that can use the same approach to better understand their game and their players. Most of the benefits regard game designers that can save time by automatically testing a game content in few minutes and obtaining accurate estimates of how a content is perceived by different players. Nevertheless, some ethical considerations need to be addressed. 

An emerging risk we need to take into account is the excessive growth of player profiling techniques especially in tech companies. It is a recent news \cite{rosenberg_how_2018}, that Cambridge Analytica, a British political consulting firm, harvested private information from more than 50 million Facebook users without their permission, making it one of the largest data leaks in the social network’s history.
Profiling technologies raises not only ethical but also privacy, security, liability and equality issues. To better understand the risks and implications of these technologies we refer to \cite{solove_digital_2006, hildebrandt_profiling_2008}.
Privacy is one of the primary issues. Tracking and monitoring individual's behaviour may reveal personal information that the monitored individuals might not even be aware of themselves. Since in our research we track data from real players, to guarantee the privacy of the players, at King, we comply with the \acl{GDPR} (GDPR) \cite{european_union_general_2017} that has become enforceable in the European Union from 25 May 2018. For this reason, all the data are anonymized in such a way that is not possible to go back to the real player.
%  we need to take into account that being able to simulate different type of players can be maliciously used.  Predicting human actions constitute a big power that can be unfairly used. For instance, it can be used to categorize individual players or provide them personalized content with the objective of maximizing in game microtransactions rather than player experience. Even if this problem is intrinsic in the collection of individual player data and it does not arise with this work, automatic playtesting with player modeling could be used to faster check how simulated players react to specific game content. This behavior represents an improper use of our work and we discourage any future work that goes into this direction. 
As the player models get improved, we need less data about individual players to predict their behaviours. However, the intent of this thesis is improving automatic playtesting to advance player experience and gaming entertainment. Consequently, we respect and promote the code of professional ethics for modeling and simulation \cite{oren_ethics_2005} defined by T. I. Orel. 
A more tragic ethical issue has been exposed by Stephen Hawking \cite{lee_hawking:_2014} at BBC in 2014. Hawking said that the increasing effort into developing thinking machines poses a threat on humanity existence. However, he refers to a full \acs{AI} that is able to think better than humans, and it is able to recursively improve itself. On the contrary, our \acs{AI} agent is only able to perform a specific task, simulating humans while playing a puzzle game. For this reason we do not elaborate more on the threat of \acs{AI}, referring to Toby \textcite{walsh_what_2016} for a deeper reflection.  

If we expand the scope, another ethical issue regards jobs that can become obsolete with the outcome of this work. As automatic playtesting become more popular, jobs that consist of testing game content could be damaged. However, automatic playtesting regards only those jobs that consist of repetitively performing the same task many times and measuring it. These monotonous jobs can be shifted to others more suitable for humans such as developing, improving and maintaining the testing infrastructure or analyzing the test results. Furthermore, in our opinion, human testing is still the best approach when it concerns subjective or qualitative measurements like fun or player experience and it should be used in conjunction with our approach. Finally, for a more exhaustive reflection on ethics of \acs{AI} we refer to \textcite{bostrom_ethics_2014}.

The last consideration regards sustainability that is becoming a central aspect in our decisions and  our lifestyles. In 2015, governments, companies and civil society defined the 17 sustainable and development goals with targets for the next 15 years. Our work is beneficial for both the goal number 8 \textit{“decent work and economic growth”} because it removes the repetitive component in the human testing work and for the goal number 12 \textit{“responsible consumption and production”} because it reduces the number of iterations needed to adjust and test a game content, diminishing the computational resources used and therefore the consumed energy.

\section{Methodology and Method}
Our work is inspired by previous researches in the automatic playtesting field as described in the Section \ref{Background_intro}. In this research we use an inductive \cite{kumar_selecting_2011} and data-driven approach because we base our decisions on analysis of player data and we try to generalize from these examples. We use a quantitative method \cite{creswell_research_2018} since we statistically test our results and because we want our outcomes to be comparable with previous researches. We attempt to create a framework that extracts the player strategies from gameplay data and simulates them. Data is collected by tracking some of the real players while playing the \textit{Candy Crush Saga} game for a period of three months. We use player modeling together with a \acs{CNN}-based approach to model and simulate gameplay of various groups of players. Subsequently, simulating different strategies, we estimate the human SR on new levels and we compare our approach with the previous state-of-the-art \cite{eisen_simulating_2017}. Finally, we use the defined player models to characterize levels. 

From a philosophical point of view, we aim to develop agents that are able to "act intelligently", or more concretely, to play a specific game "intelligently". In our case, we define \textit{intelligence} as the capability of the agent to simulate a human strategy and as a consequence to imitate \textit{human thinking}. We do not aim to develop super-human agents but we are interested modeling various types of real players, including players with low performances or players that use non-optimal strategies. Our work can be considered as an example of "weak AI" since the scope of our agents is limited to a specific and narrow task. In contrast, the "strong AI" concept refers to the development of machines with the ability of apply \textit{intelligence} to any problem and they are not limited to one specific task. 
% Therefore, answering if the agent has a mind, mental states or a \textit{"general thinking"} is behind the scope of this thesis.

\section{Delimitations}\label{delimitations}


Within the scope of this thesis, player modeling is limited to the study of the players' behaviour in terms of strategies used, defined by different policies to select actions in the game, and skills possessed by each player that allow them to solve the levels. At this moment, we do not consider the affective and psychological perception of the players that often requires specific and intrusive systems to be measured and modeled. In spite of that, a measure of the perceived entertainment could be worth when designing the game.

Despite the generic nature of the question posted in the thesis, we focus solely on the game \textit{Candy Crush Saga}, a match-three puzzle game developed by King in 2012, as a test case to answer our research question. However, the methods and analysis of this work are general and can be easily extended to similar games e.g. \textit{Farm Heroes Saga} or \textit{Bejeweled}, and they might be valuable even for other kinds of games. 
\textit{Candy Crush Saga} can be considered as a casual match-three puzzle game. In a match-three puzzle game the player has to manipulate tiles in order to create a sequence of three or more adjacent elements of the same type. Furthermore, \textit{Candy Crush Saga} is a non-deterministic game because game content is randomly generated during the game. It mainly consists of a series of levels to be fulfilled in sequence. On each level, the player has different objectives to reach. Almost all the levels are limited by a maximum number of available moves to reach the objectives. However, there are few levels (4.73\% of total levels) that are limited by time. This type of levels are not considered in this research since a different approach to model them would be necessary. Moreover, in the game there are few levels (2.83\% of the total levels) that contain a special item called "Candy Frog". Differently from any other element, the "Candy Frog" can be moved to any cell in the game board. Since a large number of additional actions would be necessary only to model this element, we decide to not consider levels that contain the "Candy Frog" in our research. Finally, the levels in \textit{Candy Crush Saga} are \acf{np-hard} problems as demonstrated by Toby Walsh in \cite{walsh_candy_2014}, meaning that in the worst case, solving them requires exponential time.
Additionally, this type of game, compared to other types e.g. shooting, adventure or managerial games, introduces more complexity in the player modeling stage due to the fact that a strategy cannot be easily described by words or using a high-level abstraction. In fact, in this thesis, the word "strategy" refers to a policy represented by a model, in our case a \acs{CNN}. In a sense, the policy function approximated by the \acs{CNN} is \textit{latent} since it cannot be observed directly. Following the taxonomy defined in \cite{holmgard_generative_2014-1}, the intent of player modeling in this research is on prediction and reproduction without necessary answering \textit{why} players exhibit certain behaviours. As a consequence we do not directly focus on description or interpretation of the generated player models.


\section{Outline}
The rest of this thesis is structured in the following way:
Chapter \ref{chap:background} presents the background and illustrates the related work in the area of this thesis. Chapter \ref{chap:method} describes the method and methodologies used during this research. Chapter \ref{chap:results} describes the results of our attempts to improve automatic playtesting using player modeling. Chapter \ref{chap:discussion} discusses interpretation, implications, consequences for the field and future work. Finally, conclusions are reported in Chapter \ref{chap:conclusions}.
